# -*- coding: utf-8 -*-
"""TP5-Mahya FARAJI.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I3SfJWXJdSb7V-TJI1JM14gpLqHaB4DK

# Introduction à la Science de Données
# TP5 - Clustering et visualisation

Ce TP s'intéresse à la visualisation à l'aide de cartes, ainsi qu'au clustering. Les données viennent d'une compétition kaggle sur le [niveau de bonheur dans le monde](https://www.kaggle.com/unsdsn/world-happiness) (si, si, ...)

## Indice de Bonheur
Dans un premier temps, il faut récupérer les données stockées dans le fichier "2017.csv". Comme d'habitude, on commence par regarder les données et leur description statistique :
"""

import pandas as pd
from google.colab import files


uploaded = files.upload()

import io
donnees = pd.read_csv(io.BytesIO(uploaded['2017.csv']))
#donnees = pd.read_csv("2017.csv")
donnees.describe()
print (donnees)


clean = donnees.drop(['Happiness.Rank','Whisker.high','Whisker.low'], axis=1) 
clean.describe()

"""On commence par regarder les corrélations entre les 8 attributs restants et le score de bonheur :"""

cor = clean.corr() # calcul de la matrice de correlation 
print (cor)

import matplotlib.pyplot as plt
# %matplotlib inline

plt.matshow(cor)
plt.xticks(range(len(cor.columns)), cor.columns, rotation=90)
plt.yticks(range(len(cor.columns)), cor.columns)

"""Chaque variable est le plus corrélée avec elle-même (bien evidemment), 
et comme nous voyons dans la figure ci-dessus la diagonale est de couleur jaune, 
du coup pour comparer les la corrélation entre les autres colonnes on oeut garder la 
couleur jaune comme réference et plus la couleur entre 2 colonnes est proches de jaune, 
plus elles sont corrélées et plus la couleur entre deux colonne passe vers bleu moins 
elles sont corrélées. Par exemple les colonnes de Economy GDP et Family et Health and 
Life expectancy sont les plus corrélées au score du bonheur mais par contre les colonnes 
de Dystopia Residual et Truds Government Corruption sont moins corrélées au score du bonheur."""



"""Carte du monde
Dans cette partie, nous allons essayer de visualiser les données géographiquement"""

! pip3 install plotly --user

"""On peut alors afficher le score de bonheur sur la carte du monde """

import plotly.graph_objs as go
from plotly.offline import init_notebook_mode, plot, iplot

# Initialisation pour l'affichage dans un notebook
init_notebook_mode(connected=True)

# Définition d'une fonction d'affichage
def carte_monde(L, z, title, bar):
    # Création du "dictionnaire de données"
    data = dict(type = 'choropleth', 
           locations = L,
           locationmode = 'country names',
           z = z, 
           text = L,
           colorbar = dict(title=bar))

    # Création du dictionnaire de format pour la carte
    layout = dict(title = title, 
             geo = dict(showframe = False, projection = dict(type='mercator')),
             autosize = False)

    # Génération de la carte
    choromap3 = go.Figure(data = [data], layout=layout)
    # Affichage
    iplot(choromap3)
    
carte_monde(clean['Country'],clean['Happiness.Score'], "Indice de bonheur 2017","Bonheur")



## Clustering

"""
1. Stocker dans une variable *X* les 7 attributs descriptifs
2. Réaliser un clustering à l'aide de l'algorithme des k-means sur l'ensemble de ces données, avec un *k* raisonable.
3. Afficher les différents clusters sur un carte du monde
4. Trouver un moyen de comparer visuellement le résultat avec l'indice de bonheur (*Happiness.Score*).
"""

from sklearn.cluster import KMeans
import numpy as np

X=clean.drop(['Country','Happiness.Score'], axis=1)
print(X)
kmeans = KMeans(n_clusters=3, random_state=0).fit(X) 
kmeans.labels_
print (kmeans.labels_) 
configure_plotly_browser_state()
carte_monde(clean['Country'],kmeans.labels_, "Indice de bonheur 2017","Bonheur")


#Evaluation de la qualité

"""Pour évaluer la qualité d'un clustering, nous savons que nous devons regarder les distances intra-classes et inter-classes. 
Mais avant de faire cela, nous ne pouvons  pas nouss empêcher de regarder en détail ce que
donne notre clustering à l'aide de la partie supervisée des données.

L'idée est donc la suivante : puisque l'on connait le classement de chaque pays 
(colonne *Happiness.rank* initialement présente dans les données), on peut regarder 
pays par pays et regarder combien sont dans la bonne classe (ou dans la mauvaise). 
On va considérer qu'un pays est dans la bonne classe si le précédent dans le classement 
et le suivant sont tous les deux dans la même classe que lui. Il est dans la mauvaise 
classe sinon.
"""

mauvais=0
bien=0
for i in range (0,154):
    if kmeans.labels_[i]==kmeans.labels_[i+1] and kmeans.labels_[i]==kmeans.labels_[i-1]:
        bien+=1
    else:
        mauvais+=1
print('Les pays bien classés:',bien)
print('Les pays mal classés:', mauvais)

"""Cette façon de faire n'est toutefois pas satisfaisante : 
d'abord elle repose sur l'utilisation d'une information "supervisée" ce qui n'est 
pas possible normalement ; ensuite parce que nous ne savons pas comment le score est 
calculé : les différents critères sont sans doute pondérés pour l'obtenir. 

L'idée ici est de regarder dans un premier temps la distance moyenne des éléments dans 
chacun des clusters.
"""

import scipy 
import numpy
from scipy.spatial import distance

cluster0=[]
add0=0
counter0=0
for i in range (0,154):
  if (kmeans.labels_[i]==0):
    counter0+=1
    cluster0.append(i)
print("Cluster 0:","\t", "Nombre des pays dans Cluster 0=",counter0,",", cluster0)
for m in cluster0[:]:  
    for n in cluster0[1:]:
      add0+=numpy.linalg.norm(X.values[m] - X.values[n])
print("add Cluster 0=", add0)
print ("Moyenne distance=",add0/counter0)
print("\n")

cluster1=[]
counter1=0
add1=0
for i in range (0,154):
  if (kmeans.labels_[i]==1):
    counter1+=1
    cluster1.append(i)
print("Cluster 1:","\t", "Nombre des pays dans Cluster 1=",counter1,",", cluster1)
for m in cluster1[:]:  
    for n in cluster1[1:]:
      add1+=numpy.linalg.norm(X.values[m] - X.values[n])
print("add Cluster 1=", add1)
print ("Moyenne distance=",add1/counter1)
print("\n")

cluster2=[]
counter2=0
add2=0
for i in range (0,154):
  if (kmeans.labels_[i]==2):
    counter2+=1
    cluster2.append(i)
print("Cluster 2:","\t", "Nombre des pays dans Cluster 2=",counter2,",", cluster2)
for m in cluster2[:]:  
    for n in cluster2[1:]:
      add2+=numpy.linalg.norm(X.values[m] - X.values[n])
print("add Cluster 2=", add2)
print ("Moyenne distance=",add2/counter2)

"""Il faut maintenant calculer la distance entre cluster. Pour cela, une façon de faire 
est de calculer le barycentre de chacun des cluster, puis de calculer la distance entre 
chacun de ces centre de gravité deux par deux."""

from sklearn.cluster import KMeans
import numpy as np

#calculer les barycentres 
kmeans = KMeans(n_clusters=3, random_state=0).fit(X)
print ("Les centres:","\n", kmeans.cluster_centers_) 

#calculer la distance entre les barycentres 

p=0
for p in range (len(kmeans.cluster_centers_)-1):
    print(numpy.linalg.norm(kmeans.cluster_centers_[p] - kmeans.cluster_centers_[p+1]))

"""Finalement, nous pouvons regader chacun des pays que nous avions noté comme "mal classé" précédemment, 
et regarder leur distance avec chacun des barycentres."""

"""Ces pays se trouves avec une distance plus ou moins égales à 2 barycentres ou même plus du barycentre, 
en fait c'est exactement cette distance que cause le mal classement. Avec un tout petit changement dans nos 
calcules, un pays peut tomber d'un cluster à l'autre, mais on le veut pas."""